#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GLM-4 åƒæ•¸è‡ªå‹•æ¸¬è©¦ç³»çµ±

æ¸¬è©¦ä¸åŒåƒæ•¸çµ„åˆå° GLM-4 å¤§ç¶±ç”Ÿæˆå“è³ªçš„å½±éŸ¿
é‡å° GLM-4 ä½œç‚º Architect çš„æœ€ä½³åƒæ•¸é…ç½®
æ•´åˆ GLM-4 ç‰¹æœ‰è©•ä¼°æŒ‡æ¨™ï¼šä¸­æ–‡æµæš¢åº¦ã€æ–‡åŒ–åº•è˜Šã€å‰µæ„æ€§ã€é‚è¼¯é€£è²«æ€§
"""

# è·¯å¾‘è¨­ç½®ï¼šå°‡çˆ¶ç›®éŒ„æ·»åŠ åˆ° sys.path ä»¥ä¾¿å°å…¥é …ç›®æ¨¡çµ„
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import os
import json
import re
import time
import argparse
from datetime import datetime
from typing import Dict, List, Tuple
import logging
from dotenv import load_dotenv

from test_r1_params_enhanced import R1ParamsTesterEnhanced
from core.api_client import SiliconFlowClient
from config import MODEL_ROLES

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# ğŸ¯ GLM-4 åƒæ•¸æ¸¬è©¦çŸ©é™£
PARAM_MATRIX_GLM4 = {
    'temperature': [0.5, 0.6, 0.7, 0.8, 0.9],      # GLM-4 é©åˆç¨é«˜æº«åº¦
    'top_p': [0.85, 0.9, 0.95],                    # ä¿æŒå¤šæ¨£æ€§
    'repetition_penalty': [1.0, 1.05, 1.1, 1.15],  # é˜²æ­¢é‡è¤‡
    'max_tokens': [4000, 6000, 8000]               # GLM-4 ä¸éœ€è¦é ç•™ <think>
}

# å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆ10 çµ„é—œéµåƒæ•¸ï¼‰
QUICK_TEST_PARAMS_GLM4 = [
    # ç•¶å‰é…ç½®
    {'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.1, 'max_tokens': 6000},

    # é«˜å‰µæ„é…ç½®
    {'temperature': 0.8, 'top_p': 0.95, 'repetition_penalty': 1.05, 'max_tokens': 6000},
    {'temperature': 0.9, 'top_p': 0.9, 'repetition_penalty': 1.0, 'max_tokens': 6000},

    # å¹³è¡¡é…ç½®
    {'temperature': 0.7, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 6000},
    {'temperature': 0.6, 'top_p': 0.9, 'repetition_penalty': 1.1, 'max_tokens': 6000},

    # ä¿å®ˆé…ç½®
    {'temperature': 0.5, 'top_p': 0.85, 'repetition_penalty': 1.15, 'max_tokens': 6000},
    {'temperature': 0.6, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 6000},

    # Token é•·åº¦æ¸¬è©¦
    {'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.1, 'max_tokens': 4000},
    {'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.1, 'max_tokens': 8000},

    # æ¥µç«¯æ¸¬è©¦
    {'temperature': 0.9, 'top_p': 0.95, 'repetition_penalty': 1.0, 'max_tokens': 8000},
]


class GLM4ParamsTester(R1ParamsTesterEnhanced):
    """GLM-4 åƒæ•¸æ¸¬è©¦å™¨ï¼ˆç¹¼æ‰¿ R1 æ¸¬è©¦å™¨ä¸¦æ“´å±•ï¼‰"""

    def __init__(self, api_key: str, quick_mode: bool = False, enable_ai_review: bool = True,
                 compare_with_r1: bool = False, debug_mode: bool = False):
        super().__init__(api_key, quick_mode, enable_ai_review)
        self.compare_with_r1 = compare_with_r1
        self.r1_best_result = None  # ç”¨æ–¼å°æ¯”
        self.debug_mode = debug_mode  # è¨ºæ–·æ¨¡å¼

        # è¦†è“‹è¼¸å‡ºç›®éŒ„ï¼ˆç›¸å°æ–¼é …ç›®æ ¹ç›®éŒ„ï¼‰
        project_root = Path(__file__).parent.parent
        self.output_dir = str(project_root / "test_results" / "glm4")
        os.makedirs(f"{self.output_dir}/outlines", exist_ok=True)
        os.makedirs(f"{self.output_dir}/ai_reviews", exist_ok=True)

    def generate_param_combinations(self) -> List[Dict]:
        """ç”Ÿæˆ GLM-4 å°ˆç”¨åƒæ•¸çµ„åˆ"""
        if self.quick_mode:
            return QUICK_TEST_PARAMS_GLM4

        # ç”Ÿæˆæ‰€æœ‰çµ„åˆï¼ˆä½¿ç”¨ GLM-4 åƒæ•¸çŸ©é™£ï¼‰
        from itertools import product
        keys = PARAM_MATRIX_GLM4.keys()
        values = PARAM_MATRIX_GLM4.values()
        combinations = []

        for combo in product(*values):
            param_dict = dict(zip(keys, combo))
            combinations.append(param_dict)

        return combinations

    def evaluate_quality(self, outline: str, params: Dict) -> Dict:
        """è©•ä¼°å¤§ç¶±å“è³ªï¼ˆGLM-4 å°ˆç”¨ - å¢åŠ ç‰¹æœ‰æŒ‡æ¨™ï¼‰"""

        # åŸºç¤è©•ä¼°ï¼ˆç¹¼æ‰¿è‡ª R1 æ¸¬è©¦å™¨ï¼‰
        score = super().evaluate_quality(outline, params)

        # Debug: é¡¯ç¤ºåŸºç¤è©•åˆ†
        if hasattr(self, 'debug_mode') and self.debug_mode:
            print(f"\nğŸ” [DEBUG] åŸºç¤è©•åˆ†: {score.get('total_score', 0):.0f}/100")

        # GLM-4 ç‰¹æœ‰æª¢æŸ¥ï¼ˆé¡å¤– 20 åˆ†ï¼‰
        if hasattr(self, 'debug_mode') and self.debug_mode:
            print(f"ğŸ” [DEBUG] é–‹å§‹ GLM-4 ç‰¹æœ‰æª¢æŸ¥...")

        glm4_checks = {
            'chinese_fluency': self.check_chinese_fluency(outline),      # ä¸­æ–‡æµæš¢åº¦
            'cultural_depth': self.check_cultural_depth(outline),        # æ–‡åŒ–åº•è˜Š
            'creativity': self.check_creativity(outline),                # å‰µæ„æ€§
            'coherence': self.check_coherence(outline),                  # é‚è¼¯é€£è²«æ€§
        }

        # Debug: é¡¯ç¤ºå„é …æª¢æŸ¥çµæœ
        if hasattr(self, 'debug_mode') and self.debug_mode:
            print(f"ğŸ” [DEBUG] GLM-4 æª¢æŸ¥çµæœ:")
            for key, value in glm4_checks.items():
                print(f"  - {key}: {value:.2f}")

        # å°‡ GLM-4 æª¢æŸ¥çµæœæ·»åŠ åˆ° details ä¸­ï¼ˆç¢ºä¿èƒ½è¢« test_param_combination æå–ï¼‰
        score['details']['glm4_chinese_fluency'] = glm4_checks['chinese_fluency']
        score['details']['glm4_cultural_depth'] = glm4_checks['cultural_depth']
        score['details']['glm4_creativity'] = glm4_checks['creativity']
        score['details']['glm4_coherence'] = glm4_checks['coherence']

        # åŒæ™‚ä¿ç•™ glm4_checks ç”¨æ–¼å ±å‘Šç”Ÿæˆ
        score['glm4_checks'] = glm4_checks

        # è¨ˆç®— GLM-4 ç‰¹æœ‰åˆ†æ•¸
        glm4_score = sum(glm4_checks.values()) * 5
        score['glm4_score'] = glm4_score
        score['total_score'] = score['total_score'] + glm4_score

        # Debug: é¡¯ç¤ºæœ€çµ‚åˆ†æ•¸
        if hasattr(self, 'debug_mode') and self.debug_mode:
            print(f"ğŸ” [DEBUG] GLM-4 ç¸½åˆ†: {glm4_score:.0f}/20")
            print(f"ğŸ” [DEBUG] æœ€çµ‚ç¸½åˆ†: {score['total_score']:.0f}/120\n")

        return score

    def check_chinese_fluency(self, outline: str) -> float:
        """
        æª¢æŸ¥ä¸­æ–‡æµæš¢åº¦ï¼ˆ0-1ï¼‰

        ç­–ç•¥ï¼šæª¢æ¸¬ç¿»è­¯è…”å’Œä¸è‡ªç„¶è¡¨é”
        """
        # ç¿»è­¯è…”æ¨¡å¼ï¼ˆæ‰£åˆ†ï¼‰
        translation_patterns = [
            (r'çš„è©±\s', 2),        # ã€Œå¦‚æœ...çš„è©±ã€ï¼ˆif...ï¼‰ç¿»è­¯è…”
            (r'é€²è¡Œäº†?\s*\w+çš„', 3),  # ã€Œé€²è¡Œç ”ç©¶çš„ã€å†—é¤˜çµæ§‹
            (r'æ˜¯\s*\w+çš„', 2),     # ã€Œæ˜¯...çš„ã€éåº¦ä½¿ç”¨
            (r'å°æ–¼\s*\w+ä¾†èªª', 2),  # ã€Œå°æ–¼...ä¾†èªªã€ï¼ˆfor...ï¼‰ç¿»è­¯è…”
        ]

        penalty = 0
        for pattern, weight in translation_patterns:
            matches = len(re.findall(pattern, outline))
            if matches > 5:
                penalty += weight * 0.05

        # è‡ªç„¶ä¸­æ–‡è¡¨é”ï¼ˆåŠ åˆ†ï¼‰
        natural_patterns = [
            r'[\u4e00-\u9fff]{1,3}[è‘—äº†é]',  # å‹•è©åŠ©è©ï¼ˆè‘—ã€äº†ã€éï¼‰
            r'[\u4e00-\u9fff]{2,4}åœ°\s*[\u4e00-\u9fff]{2,4}',  # å‰¯è©ã€Œåœ°ã€çµæ§‹
        ]

        bonus = 0
        for pattern in natural_patterns:
            matches = len(re.findall(pattern, outline))
            if matches > 10:
                bonus += 0.1

        return max(0, min(1.0, 1.0 - penalty + bonus))

    def check_cultural_depth(self, outline: str) -> float:
        """
        æª¢æŸ¥æ–‡åŒ–åº•è˜Šï¼ˆ0-1ï¼‰

        ç­–ç•¥ï¼šæª¢æ¸¬ä¸­æ–‡æˆèªã€å…¸æ•…ã€è©©è©å¼•ç”¨
        """
        score = 0

        # å››å­—æˆèªæª¢æ¸¬
        idiom_pattern = r'[\u4e00-\u9fff]{4}(?=[ï¼Œã€‚ï¼šã€ï¼ï¼Ÿ])'
        idioms = re.findall(idiom_pattern, outline)

        # éæ¿¾æ‰éæˆèªï¼ˆç°¡å–®å•Ÿç™¼å¼ï¼‰
        common_non_idioms = {'ç¬¬ä¸€ç« ', 'ç¬¬äºŒç« ', 'ç¬¬ä¸‰ç« ', 'ç¬¬å››ç« ', 'ç¬¬äº”ç« ',
                            'é€™å€‹æ™‚å€™', 'é‚£å€‹æ™‚å€™', 'æ‰€æœ‰äººéƒ½', 'æ¯å€‹äººéƒ½'}

        valid_idioms = [i for i in idioms if i not in common_non_idioms]

        # æˆèªæ•¸é‡è©•åˆ†
        if len(valid_idioms) >= 5:
            score += 0.5
        elif len(valid_idioms) >= 3:
            score += 0.3
        elif len(valid_idioms) >= 1:
            score += 0.1

        # æ–‡åŒ–å…ƒç´ é—œéµè©
        cultural_keywords = [
            'è©©', 'è©', 'å…¸', 'å¤', 'å‚³èªª', 'ç¥è©±', 'å²è¨˜',
            'å„’', 'é“', 'ç¦ª', 'å¢¨', 'æ³•', 'é™°é™½', 'äº”è¡Œ',
            'æ˜¥ç§‹', 'æˆ°åœ‹', 'æ¼¢å”', 'å®‹æ˜', 'æ¸…ä»£'
        ]

        cultural_count = sum(1 for keyword in cultural_keywords if keyword in outline)
        if cultural_count >= 3:
            score += 0.3
        elif cultural_count >= 1:
            score += 0.2

        return min(1.0, score)

    def check_creativity(self, outline: str) -> float:
        """
        æª¢æŸ¥å‰µæ„æ€§ï¼ˆ0-1ï¼‰

        ç­–ç•¥ï¼šæª¢æ¸¬æ˜¯å¦é¿é–‹å¸¸è¦‹å¥—è·¯ï¼Œæœ‰ç¨ç‰¹è¨­å®š
        """
        # å¸¸è¦‹å¥—è·¯é—œéµè©ï¼ˆæ‰£åˆ†ï¼‰
        cliche_keywords = {
            # çˆ›å¤§è¡—çš„è¨­å®š
            'æœ«æ—¥': 0.15, 'å–ªå±': 0.15, 'ç©¿è¶Š': 0.2, 'é‡ç”Ÿ': 0.2, 'ç³»çµ±': 0.25,
            'ç•°èƒ½': 0.15, 'ä¿®ä»™': 0.15, 'ç„å¹»': 0.15, 'éœ¸ç¸½': 0.2, 'ç”œå¯µ': 0.2,
            'çˆ½æ–‡': 0.25, 'æ‰“è‡‰': 0.2, 'é€†è¥²': 0.15, 'å»¢æŸ´': 0.15, 'å¤©æ‰': 0.1,
            # é™³è©æ¿«èª¿
            'é¾å‚²å¤©': 0.3, 'ç‘ªéº—è˜‡': 0.3, 'é‡‘æ‰‹æŒ‡': 0.25, 'ä¸»è§’å…‰ç’°': 0.2
        }

        penalty = 0
        for keyword, weight in cliche_keywords.items():
            if keyword in outline:
                penalty += weight

        # å‰µæ„å…ƒç´ ï¼ˆåŠ åˆ†ï¼‰
        creative_elements = {
            # ç¨ç‰¹è¨­å®š
            'æ‚–è«–': 0.15, 'å¤šå…ƒå®‡å®™': 0.15, 'é‡å­': 0.1, 'æ„è­˜ä¸Šå‚³': 0.15,
            'åŸºå› ç·¨è¼¯': 0.1, 'äººå·¥æ¼”åŒ–': 0.15, 'ç†µ': 0.1, 'ç¶­åº¦': 0.1,
            # è¤‡é›œä¸»é¡Œ
            'é“å¾·å›°å¢ƒ': 0.2, 'å€«ç†': 0.15, 'å“²å­¸': 0.15, 'å­˜åœ¨ä¸»ç¾©': 0.2,
            'è‡ªç”±æ„å¿—': 0.2, 'æ±ºå®šè«–': 0.15
        }

        bonus = 0
        for keyword, weight in creative_elements.items():
            if keyword in outline:
                bonus += weight

        # è¨ˆç®—æœ€çµ‚å‰µæ„åˆ†ï¼ˆåŸºæº– 0.5ï¼‰
        creativity_score = 0.5 - penalty + bonus

        return max(0, min(1.0, creativity_score))

    def check_coherence(self, outline: str) -> float:
        """
        æª¢æŸ¥é‚è¼¯é€£è²«æ€§ï¼ˆ0-1ï¼‰

        ç­–ç•¥ï¼š
        1. ç« ç¯€é–“å› æœé—œä¿‚
        2. è§’è‰²è¡Œç‚ºä¸€è‡´æ€§
        3. æ™‚é–“ç·šåˆç†æ€§
        """
        # æå–ç« ç¯€æ¨™é¡Œå’Œå¤§ç¶±
        chapters = re.findall(r'ç¬¬(\d+)ç« [ï¼š:]\s*(.+?)\n.*?outline["\']:\s*["\'](.+?)["\']',
                             outline, re.DOTALL)

        if len(chapters) < 3:
            return 0.5  # ç« ç¯€å¤ªå°‘ï¼Œç„¡æ³•åˆ¤æ–·

        score = 0.5  # åŸºæº–åˆ†

        # æª¢æŸ¥ 1: ç« ç¯€é‚è¼¯è©
        coherence_words = ['å› æ­¤', 'ç„¶è€Œ', 'æ¥è‘—', 'éš¨å¾Œ', 'æœ€çµ‚', 'æ–¼æ˜¯', 'ä½†æ˜¯', 'æ‰€ä»¥']
        coherence_count = sum(outline.count(word) for word in coherence_words)

        if coherence_count >= 5:
            score += 0.3
        elif coherence_count >= 3:
            score += 0.2

        # æª¢æŸ¥ 2: æ™‚é–“æ¨™è¨˜
        time_markers = ['ç¬¬ä¸€å¤©', 'ç¬¬äºŒå¤©', 'ä¸‰å¤©å¾Œ', 'ä¸€é€±å¾Œ', 'åŒæ™‚', 'æ­¤æ™‚', 'ç•¶æ™‚', 'éš¨å¾Œ']
        time_count = sum(outline.count(marker) for marker in time_markers)

        if time_count >= 3:
            score += 0.2

        return min(1.0, score)

    def load_r1_best_result(self):
        """è¼‰å…¥ R1 æœ€ä½³çµæœï¼ˆç”¨æ–¼å°æ¯”ï¼‰"""
        if not self.compare_with_r1:
            return

        logger.info("ğŸ” è¼‰å…¥ DeepSeek R1 æœ€ä½³åƒæ•¸çµæœ...")

        try:
            # è®€å– R1 æœ€æ–°å ±å‘Š
            r1_report_path = "test_results/r1_params_test_report_latest_auto.md"

            if not os.path.exists(r1_report_path):
                logger.warning(f"æœªæ‰¾åˆ° R1 å ±å‘Š: {r1_report_path}")
                return

            # è§£æå ±å‘Šç²å–æœ€ä½³é…ç½®
            with open(r1_report_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–å† è»åƒæ•¸
            champion_match = re.search(r"'temperature':\s*([\d.]+).*?'top_p':\s*([\d.]+).*?"
                                      r"'repetition_penalty':\s*([\d.]+).*?'max_tokens':\s*(\d+)",
                                      content, re.DOTALL)

            if champion_match:
                self.r1_best_result = {
                    'params': {
                        'temperature': float(champion_match.group(1)),
                        'top_p': float(champion_match.group(2)),
                        'repetition_penalty': float(champion_match.group(3)),
                        'max_tokens': int(champion_match.group(4))
                    },
                    'model': 'DeepSeek R1'
                }

                # æå–åˆ†æ•¸
                score_match = re.search(r'è‡ªå‹•è©•åˆ†[ï¼š:]\s*(\d+)/100', content)
                if score_match:
                    self.r1_best_result['auto_score'] = int(score_match.group(1))

                logger.info(f"âœ… R1 æœ€ä½³é…ç½®: temp={self.r1_best_result['params']['temperature']}, "
                          f"score={self.r1_best_result.get('auto_score', '?')}/100")
            else:
                logger.warning("ç„¡æ³•è§£æ R1 å ±å‘Šä¸­çš„åƒæ•¸")

        except Exception as e:
            logger.error(f"è¼‰å…¥ R1 çµæœå¤±æ•—: {e}")

    def run_full_test(self):
        """åŸ·è¡Œå®Œæ•´æ¸¬è©¦ï¼ˆæ•´åˆ R1 å°æ¯”ï¼‰"""
        self.start_time = time.time()

        print("\n" + "="*60)
        print("ğŸ§ª GLM-4 åƒæ•¸è‡ªå‹•æ¸¬è©¦ç³»çµ±")
        print("="*60)

        # è¼‰å…¥ R1 æœ€ä½³çµæœï¼ˆå¦‚æœéœ€è¦å°æ¯”ï¼‰
        if self.compare_with_r1:
            self.load_r1_best_result()

        # ç”Ÿæˆåƒæ•¸çµ„åˆ
        param_combinations = self.generate_param_combinations()
        total = len(param_combinations)

        mode_name = "å¿«é€Ÿæ¸¬è©¦" if self.quick_mode else "å®Œæ•´æ¸¬è©¦"
        ai_status = "å•Ÿç”¨" if self.enable_ai_review else "åœç”¨"

        print(f"\næ¨¡å¼: {mode_name}")
        print(f"AI è©•å¯©: {ai_status}")
        print(f"ç¸½æ¸¬è©¦çµ„åˆæ•¸: {total}")

        if self.compare_with_r1 and self.r1_best_result:
            print(f"å°æ¯”åŸºæº–: DeepSeek R1 (åˆ†æ•¸: {self.r1_best_result.get('auto_score', '?')}/100)")

        if self.enable_ai_review:
            print(f"é è¨ˆæ™‚é–“: {total * 1.5:.0f}-{total * 3:.0f} åˆ†é˜ï¼ˆå« AI è©•å¯©ï¼‰")
        else:
            print(f"é è¨ˆæ™‚é–“: {total * 0.5:.0f}-{total * 1:.0f} åˆ†é˜")

        print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # åŸ·è¡Œæ¸¬è©¦ï¼ˆä½¿ç”¨çˆ¶é¡çš„æ¸¬è©¦æµç¨‹ï¼‰
        super().run_full_test()

    def build_enhanced_markdown_report(self, sorted_results: List[Dict]) -> str:
        """ç”Ÿæˆ GLM-4 å¢å¼·å ±å‘Šï¼ˆå« R1 å°æ¯”ï¼‰"""
        mode_name = "å¿«é€Ÿæ¸¬è©¦" if self.quick_mode else "å®Œæ•´æ¸¬è©¦"
        ai_status = "AI è©•å¯©" if self.enable_ai_review else "è‡ªå‹•è©•ä¼°"
        total_time = time.time() - self.start_time if self.start_time else 0

        # ç¢ºä¿æ‰€æœ‰çµæœéƒ½æœ‰ total_scoreï¼ˆå…¼å®¹ --no-ai æ¨¡å¼ï¼‰
        for result in sorted_results:
            if 'total_score' not in result:
                result['total_score'] = result.get('auto_score', 0) + result.get('glm4_score', 0)

        report = f"""# GLM-4 åƒæ•¸æ¸¬è©¦å ±å‘Šï¼ˆ{ai_status}ç‰ˆï¼‰

## æ¸¬è©¦æ™‚é–“
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ¸¬è©¦é…ç½®
- æ¸¬è©¦æ¨¡å¼: {mode_name}
- è©•ä¼°æ–¹å¼: {ai_status}
- ç¸½æ¸¬è©¦çµ„åˆ: {len(self.results)} çµ„
- ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜

"""

        # GLM-4 vs R1 å°æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.compare_with_r1 and self.r1_best_result:
            report += "## ğŸ†š GLM-4 vs DeepSeek R1 å°æ¯”\n\n"

            best_glm4 = sorted_results[0]

            report += "| æŒ‡æ¨™ | GLM-4 (å† è») | DeepSeek R1 (æœ€ä½³) | å„ªå‹¢ |\n"
            report += "|------|--------------|-------------------|------|\n"

            # ä¸­è‹±æ··é›œç‡
            glm4_mixed = best_glm4['auto_details'].get('mixed_language', 0)
            report += f"| ä¸­è‹±æ··é›œç‡ | {(1-glm4_mixed)*100:.0f}% | ~60% | GLM-4 âœ… |\n"

            # ä¸­æ–‡æµæš¢åº¦
            glm4_fluency = best_glm4.get('glm4_checks', {}).get('chinese_fluency', 0)
            report += f"| ä¸­æ–‡æµæš¢åº¦ | {glm4_fluency*100:.0f}% | N/A | GLM-4 âœ… |\n"

            # æ–‡åŒ–åº•è˜Š
            glm4_culture = best_glm4.get('glm4_checks', {}).get('cultural_depth', 0)
            report += f"| æ–‡åŒ–åº•è˜Š | {glm4_culture*100:.0f}% | N/A | GLM-4 âœ… |\n"

            # å‰µæ„æ€§
            glm4_creativity = best_glm4.get('glm4_checks', {}).get('creativity', 0)
            report += f"| å‰µæ„æ€§ | {glm4_creativity*100:.0f}% | ~70% | {'GLM-4' if glm4_creativity > 0.7 else 'R1'} âœ… |\n"

            # ç¸½åˆ†å°æ¯”
            glm4_total = best_glm4['total_score']
            r1_total = self.r1_best_result.get('auto_score', 0)
            report += f"| ç¸½åˆ† | {glm4_total:.0f}/120 | {r1_total}/100 | {'GLM-4' if glm4_total > r1_total else 'R1'} âœ… |\n"

            report += "\n"

        # Top 3 çµæœ
        report += "## ğŸ† æœ€çµ‚æ’å\n\n"

        medals = ['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰']
        rankings = ['å† è»', 'äºè»', 'å­£è»']

        for i, (medal, ranking) in enumerate(zip(medals, rankings)):
            if i >= len(sorted_results):
                break

            result = sorted_results[i]
            params = result['params']

            report += f"### {medal} {ranking}\n\n"
            report += "**åƒæ•¸é…ç½®**ï¼š\n"
            report += "```python\n"
            report += "{\n"
            report += f"    'temperature': {params['temperature']},\n"
            report += f"    'top_p': {params['top_p']},\n"
            report += f"    'repetition_penalty': {params['repetition_penalty']},\n"
            report += f"    'max_tokens': {params['max_tokens']}\n"
            report += "}\n"
            report += "```\n\n"

            report += "**è©•åˆ†è©³æƒ…**ï¼š\n"
            report += f"- åŸºç¤è‡ªå‹•è©•åˆ†ï¼š{result['auto_score']:.0f}/100\n"
            report += f"- GLM-4 ç‰¹æœ‰è©•åˆ†ï¼š{result.get('glm4_score', 0):.0f}/20\n"
            report += f"- ç¸½åˆ†ï¼š{result['total_score']:.0f}/120\n\n"

            # GLM-4 ç‰¹æœ‰æŒ‡æ¨™
            if 'glm4_checks' in result:
                checks = result['glm4_checks']
                report += "**GLM-4 ç‰¹æœ‰æŒ‡æ¨™**ï¼š\n"
                report += f"- ä¸­æ–‡æµæš¢åº¦ï¼š{checks['chinese_fluency']*100:.0f}%\n"
                report += f"- æ–‡åŒ–åº•è˜Šï¼š{checks['cultural_depth']*100:.0f}%\n"
                report += f"- å‰µæ„æ€§ï¼š{checks['creativity']*100:.0f}%\n"
                report += f"- é‚è¼¯é€£è²«æ€§ï¼š{checks['coherence']*100:.0f}%\n\n"

            if self.enable_ai_review and result.get('ai_reviews'):
                report += f"- AI è©•åˆ†ï¼ˆä¸­ä½æ•¸ï¼‰ï¼š{result['ai_score']:.0f}/100\n"
                report += f"- ç¶œåˆåˆ†æ•¸ï¼š{result['combined_score']:.1f}/100\n"
                report += f"- AI ä¸€è‡´æ€§ï¼š{result['ai_agreement']}\n\n"

        # è©³ç´°æ¸¬è©¦çµæœè¡¨æ ¼
        report += "## ğŸ“Š è©³ç´°æ¸¬è©¦çµæœ\n\n"
        report += "| æ’å | temp | top_p | rep | max_tok | åŸºç¤ | GLM-4 | ç¸½åˆ† | æµæš¢ | æ–‡åŒ– | å‰µæ„ | é€£è²« |\n"
        report += "|------|------|-------|-----|---------|------|-------|------|------|------|------|------|\n"

        for i, result in enumerate(sorted_results, 1):
            params = result['params']
            checks = result.get('glm4_checks', {})

            report += f"| {i} | {params['temperature']} | {params['top_p']} | "
            report += f"{params['repetition_penalty']} | {params['max_tokens']} | "
            report += f"{result['auto_score']:.0f} | {result.get('glm4_score', 0):.0f} | "
            report += f"{result['total_score']:.0f} | "
            report += f"{checks.get('chinese_fluency', 0)*100:.0f} | "
            report += f"{checks.get('cultural_depth', 0)*100:.0f} | "
            report += f"{checks.get('creativity', 0)*100:.0f} | "
            report += f"{checks.get('coherence', 0)*100:.0f} |\n"

        report += "\n"

        # åƒæ•¸å½±éŸ¿åˆ†æ
        report += self.analyze_glm4_parameter_impact(sorted_results)

        # å»ºè­°é…ç½®
        if sorted_results:
            best = sorted_results[0]
            params = best['params']

            report += "## ğŸ’¡ å»ºè­°é…ç½®\n\n"
            report += "åŸºæ–¼æ¸¬è©¦çµæœï¼Œå»ºè­°ä½¿ç”¨ï¼š\n\n"
            report += "```python\n"
            report += "'architect': {\n"
            report += f"    'temperature': {params['temperature']},\n"
            report += f"    'top_p': {params['top_p']},\n"
            report += f"    'repetition_penalty': {params['repetition_penalty']},\n"
            report += f"    'max_tokens': {params['max_tokens']}\n"
            report += "}\n"
            report += "```\n\n"

            report += f"**ç†ç”±**ï¼š\n"
            report += f"- åŸºç¤è©•åˆ†ï¼š{best['auto_score']:.0f}/100ï¼ˆæ ¼å¼èˆ‡å…§å®¹å“è³ªï¼‰\n"
            report += f"- GLM-4 è©•åˆ†ï¼š{best.get('glm4_score', 0):.0f}/20ï¼ˆä¸­æ–‡ç‰¹æœ‰å„ªå‹¢ï¼‰\n"
            report += f"- ç¸½åˆ†ï¼š{best['total_score']:.0f}/120ï¼ˆç¶œåˆæœ€ä½³ï¼‰\n"

            checks = best.get('glm4_checks', {})
            report += f"- ä¸­æ–‡æµæš¢åº¦ï¼š{checks.get('chinese_fluency', 0)*100:.0f}%ï¼ˆè‡ªç„¶è¡¨é”ï¼‰\n"
            report += f"- æ–‡åŒ–åº•è˜Šï¼š{checks.get('cultural_depth', 0)*100:.0f}%ï¼ˆæˆèªå…¸æ•…ï¼‰\n"
            report += f"- å‰µæ„æ€§ï¼š{checks.get('creativity', 0)*100:.0f}%ï¼ˆé¿é–‹å¥—è·¯ï¼‰\n"
            report += f"- é‚è¼¯é€£è²«æ€§ï¼š{checks.get('coherence', 0)*100:.0f}%ï¼ˆç« ç¯€å‘¼æ‡‰ï¼‰\n\n"

        # æ¸¬è©¦æ•¸æ“šèªªæ˜
        report += "## ğŸ“ æ¸¬è©¦æ•¸æ“š\n\n"
        report += f"æ‰€æœ‰æ¸¬è©¦å¤§ç¶±å·²ä¿å­˜è‡³ï¼š`{self.output_dir}/outlines/`\n\n"

        return report

    def analyze_glm4_parameter_impact(self, sorted_results: List[Dict]) -> str:
        """åˆ†æ GLM-4 åƒæ•¸å½±éŸ¿"""
        report = "## ğŸ“ˆ åƒæ•¸å½±éŸ¿åˆ†æï¼ˆGLM-4 ç¸½åˆ†ï¼‰\n\n"

        # æŒ‰åƒæ•¸åˆ†çµ„çµ±è¨ˆï¼ˆä½¿ç”¨ç¸½åˆ†ï¼‰
        temp_groups = {}
        topp_groups = {}
        rep_groups = {}
        tok_groups = {}

        for result in sorted_results:
            params = result['params']
            score = result['total_score']

            # Temperature
            temp = params['temperature']
            if temp not in temp_groups:
                temp_groups[temp] = []
            temp_groups[temp].append(score)

            # Top_P
            topp = params['top_p']
            if topp not in topp_groups:
                topp_groups[topp] = []
            topp_groups[topp].append(score)

            # Repetition Penalty
            rep = params['repetition_penalty']
            if rep not in rep_groups:
                rep_groups[rep] = []
            rep_groups[rep].append(score)

            # Max Tokens
            tok = params['max_tokens']
            if tok not in tok_groups:
                tok_groups[tok] = []
            tok_groups[tok].append(score)

        # Temperature å½±éŸ¿
        report += "### Temperature å½±éŸ¿\n\n"
        for temp in sorted(temp_groups.keys()):
            scores = temp_groups[temp]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(temp_groups[t])/len(temp_groups[t]) for t in temp_groups) else ""
            report += f"- **{temp}**: å¹³å‡åˆ† {avg_score:.1f}/120{best_mark}\n"
        report += "\n**å»ºè­°ç¯„åœ**: 0.6-0.8ï¼ˆå¹³è¡¡å‰µæ„èˆ‡ç©©å®šï¼‰\n\n"

        # Top_P å½±éŸ¿
        report += "### Top_P å½±éŸ¿\n\n"
        for topp in sorted(topp_groups.keys()):
            scores = topp_groups[topp]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(topp_groups[t])/len(topp_groups[t]) for t in topp_groups) else ""
            report += f"- **{topp}**: å¹³å‡åˆ† {avg_score:.1f}/120{best_mark}\n"
        report += "\n**å»ºè­°ç¯„åœ**: 0.85-0.95ï¼ˆä¿æŒå¤šæ¨£æ€§ï¼‰\n\n"

        # Repetition Penalty å½±éŸ¿
        report += "### Repetition Penalty å½±éŸ¿\n\n"
        for rep in sorted(rep_groups.keys()):
            scores = rep_groups[rep]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(rep_groups[r])/len(rep_groups[r]) for r in rep_groups) else ""
            report += f"- **{rep}**: å¹³å‡åˆ† {avg_score:.1f}/120{best_mark}\n"
        report += "\n**å»ºè­°ç¯„åœ**: 1.05-1.15ï¼ˆé˜²æ­¢é‡è¤‡ï¼‰\n\n"

        # Max Tokens å½±éŸ¿
        report += "### Max Tokens å½±éŸ¿\n\n"
        for tok in sorted(tok_groups.keys()):
            scores = tok_groups[tok]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(tok_groups[t])/len(tok_groups[t]) for t in tok_groups) else ""
            report += f"- **{tok}**: å¹³å‡åˆ† {avg_score:.1f}/120{best_mark}\n"
        report += "\n**å»ºè­°ç¯„åœ**: 6000ï¼ˆæœ€ä½³æ€§åƒ¹æ¯”ï¼‰\n\n"

        return report


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='GLM-4 åƒæ•¸è‡ªå‹•æ¸¬è©¦ç³»çµ±')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆ10çµ„é—œéµåƒæ•¸ï¼‰')
    parser.add_argument('--full', action='store_true', help='å®Œæ•´æ¸¬è©¦æ¨¡å¼ï¼ˆæ‰€æœ‰çµ„åˆï¼‰')
    parser.add_argument('--no-ai', action='store_true', help='åœç”¨ AI è©•å¯©ï¼ˆåƒ…è‡ªå‹•è©•ä¼°ï¼‰')
    parser.add_argument('--compare-with-r1', action='store_true', help='èˆ‡ DeepSeek R1 å°æ¯”æ¸¬è©¦')
    parser.add_argument('--debug', action='store_true', help='å•Ÿç”¨è¨ºæ–·æ¨¡å¼ï¼ˆé¡¯ç¤ºè©³ç´°è©•åˆ†éç¨‹ï¼‰')
    args = parser.parse_args()

    # åŠ è¼‰ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    api_key = os.getenv('SILICONFLOW_API_KEY')

    if not api_key:
        print("âŒ éŒ¯èª¤: æœªæª¢æ¸¬åˆ° SILICONFLOW_API_KEY")
        print("è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½® API Key")
        return

    # ç¢ºå®šæ¸¬è©¦æ¨¡å¼
    if args.full:
        quick_mode = False
    elif args.quick:
        quick_mode = True
    else:
        # é»˜èªä½¿ç”¨å¿«é€Ÿæ¨¡å¼
        quick_mode = True
        print("ğŸ’¡ æç¤º: æœªæŒ‡å®šæ¨¡å¼ï¼Œä½¿ç”¨å¿«é€Ÿæ¸¬è©¦æ¨¡å¼")
        print("   ä½¿ç”¨ --full å¯é€²è¡Œå®Œæ•´æ¸¬è©¦")
        print("   ä½¿ç”¨ --quick æ˜ç¢ºæŒ‡å®šå¿«é€Ÿæ¸¬è©¦")
        print("   ä½¿ç”¨ --no-ai åœç”¨ AI è©•å¯©")
        print("   ä½¿ç”¨ --compare-with-r1 èˆ‡ R1 å°æ¯”")
        print("   ä½¿ç”¨ --debug å•Ÿç”¨è¨ºæ–·æ¨¡å¼\n")

    # AI è©•å¯©ç‹€æ…‹
    enable_ai = not args.no_ai

    if not enable_ai:
        print("âš ï¸  AI è©•å¯©å·²åœç”¨ï¼Œåƒ…ä½¿ç”¨è‡ªå‹•è©•ä¼°")

    # è¨ºæ–·æ¨¡å¼æç¤º
    if args.debug:
        print("ğŸ” è¨ºæ–·æ¨¡å¼å·²å•Ÿç”¨ï¼Œå°‡é¡¯ç¤ºè©³ç´°è©•åˆ†éç¨‹\n")

    # å‰µå»ºæ¸¬è©¦å™¨ä¸¦é‹è¡Œ
    tester = GLM4ParamsTester(
        api_key,
        quick_mode=quick_mode,
        enable_ai_review=enable_ai,
        compare_with_r1=args.compare_with_r1,
        debug_mode=args.debug
    )
    tester.run_full_test()


if __name__ == "__main__":
    main()
