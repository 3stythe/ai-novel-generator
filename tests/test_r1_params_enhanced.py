#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DeepSeek R1 åƒæ•¸è‡ªå‹•æ¸¬è©¦ç³»çµ±ï¼ˆAI è©•å¯©å¢å¼·ç‰ˆï¼‰

æ¸¬è©¦ä¸åŒåƒæ•¸çµ„åˆå°å¤§ç¶±ç”Ÿæˆå“è³ªçš„å½±éŸ¿
æ•´åˆå°æŠ—å¼è©•ä¼°ã€å¤š AI æŠ•ç¥¨å’Œç›¸å°æ’åæ³•
"""

# è·¯å¾‘è¨­ç½®ï¼šå°‡çˆ¶ç›®éŒ„æ·»åŠ åˆ° sys.path
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import os
import json
import re
import time
import random
import argparse
from datetime import datetime
from itertools import product
from typing import Dict, List, Tuple
import logging
from dotenv import load_dotenv

from core.generator import NovelGenerator
from core.api_client import SiliconFlowClient
from utils.json_parser import RobustJSONParser
from config import MODEL_ROLES

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# åƒæ•¸æ¸¬è©¦çŸ©é™£
PARAM_MATRIX = {
    'temperature': [0.3, 0.4, 0.5, 0.6, 0.7],
    'top_p': [0.8, 0.85, 0.9, 0.95],
    'repetition_penalty': [1.0, 1.05, 1.1, 1.15],
    'max_tokens': [6000, 8000, 10000]
}

# å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆé—œéµåƒæ•¸çµ„åˆï¼‰
QUICK_PARAM_COMBINATIONS = [
    {'temperature': 0.3, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 8000},
    {'temperature': 0.4, 'top_p': 0.8, 'repetition_penalty': 1.1, 'max_tokens': 8000},
    {'temperature': 0.4, 'top_p': 0.85, 'repetition_penalty': 1.0, 'max_tokens': 8000},
    {'temperature': 0.4, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 6000},
    {'temperature': 0.4, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 8000},  # ç·Šæ€¥ä¿®å¾©åƒæ•¸
    {'temperature': 0.4, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 10000},
    {'temperature': 0.4, 'top_p': 0.9, 'repetition_penalty': 1.1, 'max_tokens': 8000},
    {'temperature': 0.5, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 8000},
    {'temperature': 0.5, 'top_p': 0.95, 'repetition_penalty': 1.0, 'max_tokens': 8192},  # R1 å®˜æ–¹åƒæ•¸
    {'temperature': 0.6, 'top_p': 0.85, 'repetition_penalty': 1.1, 'max_tokens': 8000},
]


class R1ParamsTesterEnhanced:
    """DeepSeek R1 åƒæ•¸æ¸¬è©¦å™¨ï¼ˆAI è©•å¯©å¢å¼·ç‰ˆï¼‰"""

    def __init__(self, api_key: str, quick_mode: bool = False, enable_ai_review: bool = True):
        self.api_key = api_key
        self.quick_mode = quick_mode
        self.enable_ai_review = enable_ai_review
        self.results = []
        # è¼¸å‡ºç›®éŒ„ï¼ˆç›¸å°æ–¼é …ç›®æ ¹ç›®éŒ„ï¼‰
        project_root = Path(__file__).parent.parent
        self.output_dir = str(project_root / "test_results")
        self.start_time = None

        # åˆå§‹åŒ– API å®¢æˆ¶ç«¯å’Œ JSON è§£æå™¨
        self.api_client = SiliconFlowClient(api_key)
        self.json_parser = RobustJSONParser()

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(f"{self.output_dir}/outlines", exist_ok=True)
        os.makedirs(f"{self.output_dir}/ai_reviews", exist_ok=True)

    def generate_param_combinations(self) -> List[Dict]:
        """ç”Ÿæˆåƒæ•¸çµ„åˆ"""
        if self.quick_mode:
            return QUICK_PARAM_COMBINATIONS

        # ç”Ÿæˆæ‰€æœ‰çµ„åˆ
        keys = PARAM_MATRIX.keys()
        values = PARAM_MATRIX.values()
        combinations = []

        for combo in product(*values):
            param_dict = dict(zip(keys, combo))
            combinations.append(param_dict)

        return combinations

    def adversarial_review(self, outline: str, model: str) -> Dict:
        """
        å°æŠ—å¼è©•ä¼°ï¼šAI æ‰®æ¼”æŒ‘å‰”ç·¨è¼¯ï¼Œæ‰¾å‡º 5 å€‹è‡´å‘½ç¼ºé™·

        Args:
            outline: å¤§ç¶±æ–‡æœ¬
            model: è©•å¯©æ¨¡å‹ï¼ˆDeepSeek/Qwen/GLMï¼‰

        Returns:
            {
                'model': 'DeepSeek',
                'fatal_flaws': [ç¼ºé™·1, ç¼ºé™·2, ...],
                'flaw_count': 5,
                'severity_score': 85  # 100 - (ç¼ºé™·æ•¸Ã—åš´é‡åº¦)
            }
        """

        prompt = f"""ä½ æ˜¯ä¸€ä½æ¥µåº¦æŒ‘å‰”çš„è³‡æ·±å°èªªç·¨è¼¯ï¼Œå°ˆé–€æ‰¾å¤§ç¶±çš„è‡´å‘½ç¼ºé™·ã€‚

ã€ä»»å‹™ã€‘
è«‹æ‰¾å‡ºé€™ä»½å¤§ç¶±çš„ 5 å€‹æœ€åš´é‡çš„å•é¡Œã€‚å¦‚æœæ‰¾ä¸åˆ° 5 å€‹ï¼Œèªªæ˜å“è³ªå¾ˆå¥½ã€‚

ã€å¤§ç¶±ã€‘
{outline[:3000]}

ã€è¦æ±‚ã€‘
1. åªåˆ—å‡ºçœŸæ­£è‡´å‘½çš„ç¼ºé™·ï¼ˆæœƒè®“è®€è€…æ”¾æ£„çš„å•é¡Œï¼‰
2. æ¯å€‹ç¼ºé™·è¦å…·é«”æŒ‡å‡ºä½ç½®å’ŒåŸå› 
3. æŒ‰åš´é‡ç¨‹åº¦æ’åºï¼ˆæœ€åš´é‡çš„æ”¾ç¬¬ä¸€ï¼‰

ã€è¼¸å‡ºæ ¼å¼ã€‘
åš´æ ¼ä½¿ç”¨ JSON æ ¼å¼ï¼š
{{
    "fatal_flaws": [
        {{"issue": "å•é¡Œæè¿°", "location": "ç¬¬Xç« ", "severity": "high/medium/low"}},
        ...
    ],
    "flaw_count": å¯¦éš›æ‰¾åˆ°çš„ç¼ºé™·æ•¸é‡,
    "overall_verdict": "æ•´é«”è©•åƒ¹ä¸€å¥è©±"
}}

ä¸è¦è¼¸å‡ºæ€è€ƒéç¨‹ï¼Œç›´æ¥è¼¸å‡º JSONã€‚
"""

        try:
            # èª¿ç”¨ APIï¼ˆä½¿ç”¨æŒ‡å®šæ¨¡å‹ï¼‰
            response = self.api_client.generate(prompt, model=model, max_tokens=2000, temperature=0.3)

            # è§£æ JSON
            result = self.json_parser.parse(response)

            # ç¢ºä¿æœ‰å¿…è¦çš„å­—æ®µ
            if 'fatal_flaws' not in result:
                result['fatal_flaws'] = []
            if 'flaw_count' not in result:
                result['flaw_count'] = len(result['fatal_flaws'])
            if 'overall_verdict' not in result:
                result['overall_verdict'] = "ç„¡è©•åƒ¹"

            # è¨ˆç®—åš´é‡åº¦åˆ†æ•¸
            severity_map = {'high': 20, 'medium': 10, 'low': 5}
            severity_penalty = sum(severity_map.get(f.get('severity', 'medium'), 10)
                                  for f in result['fatal_flaws'])

            return {
                'model': model,
                'fatal_flaws': result['fatal_flaws'],
                'flaw_count': result['flaw_count'],
                'severity_score': max(0, 100 - severity_penalty),
                'verdict': result['overall_verdict']
            }

        except Exception as e:
            logger.error(f"å°æŠ—å¼è©•å¯©å¤±æ•— ({model}): {e}")
            # è¿”å›é»˜èªå€¼
            return {
                'model': model,
                'fatal_flaws': [],
                'flaw_count': 0,
                'severity_score': 50,  # é»˜èªä¸­ç­‰åˆ†æ•¸
                'verdict': f"è©•å¯©å¤±æ•—: {str(e)[:50]}",
                'error': str(e)
            }

    def multi_ai_voting(self, outline: str) -> Dict:
        """
        å¤š AI æŠ•ç¥¨ï¼š3 å€‹æ¨¡å‹è©•åˆ†ï¼Œå–ä¸­ä½æ•¸

        Returns:
            {
                'reviews': [AI1çµæœ, AI2çµæœ, AI3çµæœ],
                'median_score': 78,
                'agreement': 'high/medium/low'
            }
        """

        # ä¸‰å€‹è©•å¯©æ¨¡å‹
        reviewers = [
            ('DeepSeek R1', MODEL_ROLES['architect']),  # é‚è¼¯æ¨ç†
            ('GLM-4', MODEL_ROLES['writer']),           # å‰µæ„å¯«ä½œ
            ('Qwen Coder', MODEL_ROLES['editor'])       # å“è³ªæª¢æŸ¥
        ]

        reviews = []

        for name, model in reviewers:
            logger.info(f"  è©•å¯©æ¨¡å‹: {name}")

            try:
                review = self.adversarial_review(outline, model)
                review['reviewer_name'] = name
                reviews.append(review)
            except Exception as e:
                logger.error(f"  è©•å¯©å¤±æ•— ({name}): {e}")
                # æ·»åŠ å¤±æ•—è¨˜éŒ„
                reviews.append({
                    'model': model,
                    'reviewer_name': name,
                    'fatal_flaws': [],
                    'flaw_count': 0,
                    'severity_score': 0,
                    'verdict': f"è©•å¯©å¤±æ•—: {str(e)[:50]}",
                    'error': str(e)
                })

        if len(reviews) < 2:
            logger.warning("è©•å¯©æ¨¡å‹ä¸è¶³ï¼Œä½¿ç”¨é»˜èªåˆ†æ•¸")
            return {
                'reviews': reviews,
                'median_score': 50,
                'agreement': 'unknown',
                'score_range': 0
            }

        # è¨ˆç®—ä¸­ä½æ•¸åˆ†æ•¸
        scores = [r['severity_score'] for r in reviews]
        sorted_scores = sorted(scores)
        median_score = sorted_scores[len(sorted_scores) // 2]

        # è¨ˆç®—ä¸€è‡´æ€§
        score_range = max(scores) - min(scores)
        if score_range <= 10:
            agreement = 'high'
        elif score_range <= 20:
            agreement = 'medium'
        else:
            agreement = 'low'

        return {
            'reviews': reviews,
            'median_score': median_score,
            'agreement': agreement,
            'score_range': score_range,
            'mean_score': sum(scores) / len(scores)
        }

    def pairwise_comparison(self, outline_a: str, outline_b: str,
                            params_a: Dict, params_b: Dict) -> Dict:
        """
        å…©å…©æ¯”è¼ƒï¼šè©•å¯© AI åˆ¤æ–·å“ªå€‹æ›´å¥½

        Returns:
            {
                'winner': 'A' or 'B',
                'reason': 'å‹å‡ºåŸå› ',
                'confidence': 'high/medium/low'
            }
        """

        prompt = f"""ä½ æ˜¯è³‡æ·±å°èªªç·¨è¼¯ï¼Œè«‹æ¯”è¼ƒä»¥ä¸‹å…©ä»½å¤§ç¶±ï¼Œåˆ¤æ–·å“ªä¸€ä»½æ›´å¥½ã€‚

ã€å¤§ç¶± Aã€‘
{outline_a[:1500]}...

ã€å¤§ç¶± Bã€‘
{outline_b[:1500]}...

ã€è©•ä¼°æ¨™æº–ã€‘
1. æƒ…ç¯€å®Œæ•´æ€§å’Œå¸å¼•åŠ›
2. è§’è‰²è¨­å®šçš„ç¨ç‰¹æ€§
3. é‚è¼¯ä¸€è‡´æ€§
4. å‰µæ„æ€§ï¼ˆæ˜¯å¦é¿é–‹è€æ¢—ï¼‰
5. æè¿°çš„å…·é«”æ€§

ã€è¼¸å‡ºæ ¼å¼ã€‘
åš´æ ¼ä½¿ç”¨ JSONï¼š
{{
    "winner": "A" or "B",
    "reason": "å‹å‡ºåŸå› ï¼ˆä¸€å¥è©±ï¼‰",
    "confidence": "high/medium/low",
    "score_diff": 5
}}
"""

        try:
            # ä½¿ç”¨ Writer æ¨¡å‹ï¼ˆGLM-4ï¼‰ä½œç‚ºè©•å¯©
            response = self.api_client.generate(prompt, model=MODEL_ROLES['writer'],
                                              max_tokens=500, temperature=0.3)
            result = self.json_parser.parse(response)

            # ç¢ºä¿æœ‰å¿…è¦çš„å­—æ®µ
            if 'winner' not in result:
                result['winner'] = 'A'  # é»˜èª
            if 'reason' not in result:
                result['reason'] = 'ç„¡ç†ç”±'
            if 'confidence' not in result:
                result['confidence'] = 'low'

            # è¨˜éŒ„çµæœ
            result['params_winner'] = params_a if result['winner'] == 'A' else params_b

            return result

        except Exception as e:
            logger.error(f"å…©å…©æ¯”è¼ƒå¤±æ•—: {e}")
            # è¿”å›é»˜èªçµæœï¼ˆéš¨æ©Ÿé¸æ“‡ï¼‰
            return {
                'winner': random.choice(['A', 'B']),
                'reason': f'æ¯”è¼ƒå¤±æ•—: {str(e)[:50]}',
                'confidence': 'low',
                'score_diff': 0,
                'params_winner': params_a,
                'error': str(e)
            }

    def rank_by_comparison(self, outlines_with_params: List[Tuple]) -> List:
        """
        é€šéå…©å…©æ¯”è¼ƒæ’åºæ‰€æœ‰å¤§ç¶±

        Args:
            outlines_with_params: [(outline, params, score_dict), ...]

        Returns:
            æ’åºå¾Œçš„åˆ—è¡¨ï¼ˆæŒ‰å‹ç‡é™åºï¼‰
        """

        n = len(outlines_with_params)
        win_count = {i: 0 for i in range(n)}

        # å…©å…©æ¯”è¼ƒ
        comparisons = 0
        max_comparisons = min(n * (n - 1) // 2, 20)  # æœ€å¤š 20 æ¬¡æ¯”è¼ƒï¼ˆæ§åˆ¶æˆæœ¬ï¼‰

        logger.info(f"\nğŸ† é–‹å§‹ç›¸å°æ’åï¼ˆ{n} å€‹å€™é¸ï¼Œæœ€å¤š {max_comparisons} æ¬¡æ¯”è¼ƒï¼‰")

        # ç”Ÿæˆæ‰€æœ‰é…å°
        pairs = [(i, j) for i in range(n) for j in range(i+1, n)]
        random.shuffle(pairs)

        for i, j in pairs[:max_comparisons]:
            outline_a, params_a, score_a = outlines_with_params[i]
            outline_b, params_b, score_b = outlines_with_params[j]

            logger.info(f"  æ¯”è¼ƒ: #{i+1} vs #{j+1} ({comparisons+1}/{max_comparisons})")

            try:
                result = self.pairwise_comparison(outline_a, outline_b, params_a, params_b)

                if result['winner'] == 'A':
                    win_count[i] += 1
                else:
                    win_count[j] += 1

                logger.info(f"  çµæœ: {result['winner']} å‹ ({result['confidence']})")

                comparisons += 1

            except Exception as e:
                logger.error(f"  æ¯”è¼ƒå¤±æ•—: {e}")

        # æŒ‰å‹ç‡æ’åº
        ranked_indices = sorted(range(n), key=lambda i: win_count[i], reverse=True)

        logger.info("\nğŸ“Š ç›¸å°æ’åçµæœ:")
        for rank, idx in enumerate(ranked_indices, 1):
            logger.info(f"  {rank}. æ¸¬è©¦ #{idx+1} - å‹å ´: {win_count[idx]}/{comparisons}")

        return [outlines_with_params[i] for i in ranked_indices]

    def test_param_combination(self, params: Dict, index: int, total: int) -> Tuple[str, Dict]:
        """æ¸¬è©¦å–®çµ„åƒæ•¸ï¼ˆæ•´åˆ AI è©•å¯©ï¼‰"""
        logger.info(f"\n{'='*60}")
        logger.info(f"æ¸¬è©¦ {index}/{total}")
        logger.info(f"åƒæ•¸: temp={params['temperature']}, top_p={params['top_p']}, "
                   f"rep={params['repetition_penalty']}, max_tok={params['max_tokens']}")
        logger.info(f"{'='*60}")

        try:
            # 1. ç”Ÿæˆå¤§ç¶±
            outline = self.generate_outline(params)

            # 2. è‡ªå‹•è©•ä¼°ï¼ˆåŸæœ‰çš„ 10 é …æª¢æŸ¥ï¼‰
            auto_score = self.evaluate_quality(outline, params)

            # 3. AI è©•å¯©ï¼ˆå°æŠ—å¼ + å¤š AI æŠ•ç¥¨ï¼‰
            if self.enable_ai_review:
                logger.info("ğŸ¤– é–‹å§‹ AI è©•å¯©...")
                ai_review = self.multi_ai_voting(outline)
            else:
                # ä¸å•Ÿç”¨ AI è©•å¯©æ™‚ä½¿ç”¨é»˜èªå€¼
                ai_review = {
                    'reviews': [],
                    'median_score': auto_score['total_score'],
                    'agreement': 'N/A',
                    'score_range': 0
                }

            # 4. ç¶œåˆåˆ†æ•¸
            final_score = {
                'params': params,
                'auto_score': auto_score['total_score'],      # è‡ªå‹•è©•åˆ†ï¼ˆ100åˆ†ï¼‰
                'ai_score': ai_review['median_score'],        # AI è©•åˆ†ï¼ˆ100åˆ†ï¼‰
                'combined_score': (auto_score['total_score'] * 0.4 +
                                  ai_review['median_score'] * 0.6) if self.enable_ai_review
                                  else auto_score['total_score'],  # ç¶œåˆï¼ˆ40%è‡ªå‹• + 60%AIï¼‰
                'ai_reviews': ai_review['reviews'],
                'ai_agreement': ai_review['agreement'],
                'auto_details': auto_score['details']
            }

            # 5. ä¿å­˜å¤§ç¶±ï¼ˆåŒ…å« AI è©•å¯©æ„è¦‹ï¼‰
            self.save_outline_with_reviews(outline, params, final_score, index)

            logger.info(f"âœ… æ¸¬è©¦å®Œæˆ")
            logger.info(f"  è‡ªå‹•åˆ†æ•¸: {final_score['auto_score']:.0f}/100")
            if self.enable_ai_review:
                logger.info(f"  AI åˆ†æ•¸: {final_score['ai_score']:.0f}/100")
                logger.info(f"  ç¶œåˆåˆ†æ•¸: {final_score['combined_score']:.1f}/100")
                logger.info(f"  AI ä¸€è‡´æ€§: {final_score['ai_agreement']}")

            return outline, final_score

        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()

            # è¿”å›ç©ºå¤§ç¶±å’Œé»˜èªåˆ†æ•¸
            return "", {
                'params': params,
                'auto_score': 0,
                'ai_score': 0,
                'combined_score': 0,
                'ai_reviews': [],
                'ai_agreement': 'N/A',
                'auto_details': {},
                'error': str(e)
            }

    def generate_outline(self, params: Dict) -> str:
        """ç”Ÿæˆå¤§ç¶±ï¼ˆä½¿ç”¨æŒ‡å®šåƒæ•¸ï¼‰"""
        # è‡¨æ™‚ä¿®æ”¹é…ç½®
        import config
        original_config = config.ROLE_CONFIGS['architect'].copy()

        try:
            # æ‡‰ç”¨æ¸¬è©¦åƒæ•¸
            config.ROLE_CONFIGS['architect'] = params

            # å‰µå»ºç”Ÿæˆå™¨
            generator = NovelGenerator(self.api_key, enable_phase2=False)

            # å‰µå»ºæ¸¬è©¦å°ˆæ¡ˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            title = f"æ¸¬è©¦å°èªª_{timestamp}"

            generator.create_project(
                title=title,
                genre="ç§‘å¹»",
                theme="AI è¦ºé†’èˆ‡äººé¡é—œä¿‚",
                total_chapters=5
            )

            # ç”Ÿæˆå¤§ç¶±
            generator.generate_outline()

            # è®€å–å¤§ç¶±
            outline_file = os.path.join(generator.project_dir, 'outline.txt')
            with open(outline_file, 'r', encoding='utf-8') as f:
                outline = f.read()

            # æ¸…ç†æ¸¬è©¦å°ˆæ¡ˆ
            import shutil
            shutil.rmtree(generator.project_dir)

            return outline

        finally:
            # æ¢å¾©åŸå§‹é…ç½®
            config.ROLE_CONFIGS['architect'] = original_config

    def evaluate_quality(self, outline: str, params: Dict) -> Dict:
        """è©•ä¼°å¤§ç¶±å“è³ªï¼ˆåŸæœ‰çš„è‡ªå‹•è©•ä¼°ï¼‰"""
        score = {
            'params': params,
            'format_score': 0,
            'content_score': 0,
            'length_score': 0,
            'total_score': 0,
            'details': {}
        }

        # æ ¼å¼å“è³ªè©•ä¼°ï¼ˆ40åˆ†ï¼‰
        think_score = self.check_no_think_tags(outline)
        star_score = self.check_no_star_placeholders(outline)
        dot_score = self.check_no_dot_placeholders(outline)
        lang_score = self.check_no_mixed_language(outline)

        score['format_score'] = (think_score + star_score + dot_score + lang_score) * 10
        score['details']['think_tags'] = think_score
        score['details']['star_placeholders'] = star_score
        score['details']['dot_placeholders'] = dot_score
        score['details']['mixed_language'] = lang_score

        # å…§å®¹å“è³ªè©•ä¼°ï¼ˆ40åˆ†ï¼‰
        unique_score = self.check_unique_titles(outline)
        concrete_score = self.check_concrete_plots(outline)
        names_score = self.check_complete_names(outline)
        repeat_score = self.check_no_repetition(outline)

        score['content_score'] = (unique_score + concrete_score + names_score + repeat_score) * 10
        score['details']['unique_titles'] = unique_score
        score['details']['concrete_plots'] = concrete_score
        score['details']['complete_names'] = names_score
        score['details']['no_repetition'] = repeat_score

        # é•·åº¦å“è³ªè©•ä¼°ï¼ˆ20åˆ†ï¼‰
        length_score = self.check_outline_length(outline)
        chapter_score = self.check_chapter_length(outline)

        score['length_score'] = (length_score + chapter_score) * 10
        score['details']['outline_length'] = length_score
        score['details']['chapter_length'] = chapter_score

        score['total_score'] = score['format_score'] + score['content_score'] + score['length_score']

        return score

    # å“è³ªæª¢æŸ¥æ–¹æ³•ï¼ˆä¿æŒèˆ‡åŸç‰ˆä¸€è‡´ï¼‰
    def check_no_think_tags(self, outline: str) -> float:
        """æª¢æŸ¥ç„¡ <think> æ¨™ç±¤ï¼ˆ0-1ï¼‰"""
        if '<think>' in outline or '</think>' in outline:
            return 0.0
        if outline.startswith('å—¯ï¼Œ') or outline.startswith('å¥½çš„ï¼Œ') or outline.startswith('é¦–å…ˆï¼Œ'):
            return 0.3
        return 1.0

    def check_no_star_placeholders(self, outline: str) -> float:
        """æª¢æŸ¥ç„¡æ˜Ÿè™Ÿä½”ä½ç¬¦ï¼ˆ0-1ï¼‰"""
        if '*********' in outline or '****' in outline:
            return 0.0
        star_count = outline.count('*')
        if star_count > 50:
            return 0.3
        elif star_count > 20:
            return 0.7
        return 1.0

    def check_no_dot_placeholders(self, outline: str) -> float:
        """æª¢æŸ¥ç„¡çœç•¥è™Ÿä½”ä½ç¬¦ï¼ˆ0-1ï¼‰"""
        if '........' in outline or '......' in outline:
            return 0.0
        dot_count = outline.count('...')
        if dot_count > 20:
            return 0.3
        elif dot_count > 10:
            return 0.7
        return 1.0

    def check_no_mixed_language(self, outline: str) -> float:
        """
        æª¢æŸ¥ç„¡ä¸­è‹±æ–‡æ··é›œï¼ˆ0-1ï¼‰

        ç­–ç•¥ï¼šæª¢æ¸¬æ‰€æœ‰é€£çºŒè‹±æ–‡å–®è©ï¼Œæ’é™¤å…è¨±çš„å°ˆæœ‰åè©
        """
        chinese_sections = re.findall(r'[\u4e00-\u9fff]+', outline)
        if not chinese_sections:
            return 0.0

        # å…è¨±çš„å°ˆæœ‰åè©ç™½åå–®ï¼ˆä¸è¨ˆå…¥æ··é›œï¼‰
        allowed_terms = {
            'ai', 'cpu', 'gpu', 'vr', 'ar', 'ml', 'nlp', 'api',  # æŠ€è¡“ç¸®å¯«
            'ok', 'no', 'yes',  # å¸¸è¦‹çŸ­è©
            'cyberpunk', 'steampunk',  # æ‹¬è™Ÿæ¨™è¨»çš„å°ˆæœ‰åè©
        }

        # æª¢æ¸¬æ‰€æœ‰é€£çºŒè‹±æ–‡å–®è©ï¼ˆ2+ å­—æ¯ï¼‰
        english_words = re.findall(r'\b[a-zA-Z]{2,}\b', outline.lower())

        # éæ¿¾æ‰ç™½åå–®è©å½™
        mixed_words = [w for w in english_words if w not in allowed_terms]

        # è¨ˆç®—æ··é›œç¨‹åº¦
        mixed_count = len(mixed_words)
        total_words = len(chinese_sections) + len(english_words)

        if total_words == 0:
            return 0.0

        mixed_ratio = mixed_count / total_words

        # è©•åˆ†æ¨™æº–ï¼ˆæ›´åš´æ ¼ï¼‰
        if mixed_ratio > 0.15:  # è¶…é 15% è‹±æ–‡ â†’ 0 åˆ†
            return 0.0
        elif mixed_ratio > 0.08:  # 8-15% â†’ 0.3 åˆ†
            return 0.3
        elif mixed_ratio > 0.03:  # 3-8% â†’ 0.6 åˆ†
            return 0.6
        elif mixed_ratio > 0:     # 1-3% â†’ 0.8 åˆ†
            return 0.8
        return 1.0  # 0% â†’ æ»¿åˆ†

    def check_unique_titles(self, outline: str) -> float:
        """æª¢æŸ¥ç« ç¯€æ¨™é¡Œæœ‰å·®ç•°ï¼ˆ0-1ï¼‰"""
        titles = re.findall(r'ç¬¬\d+ç« [ï¼š:]\s*(.+)', outline)
        if len(titles) < 3:
            return 0.5

        unique_titles = set(titles)
        uniqueness = len(unique_titles) / max(len(titles), 1)
        return uniqueness

    def check_concrete_plots(self, outline: str) -> float:
        """æª¢æŸ¥æƒ…ç¯€æè¿°å…·é«”ï¼ˆ0-1ï¼‰"""
        action_verbs = ['ç™¼ç¾', 'æ¢ç´¢', 'å°æŠ—', 'é¸æ“‡', 'çŠ§ç‰²', 'é€²å…¥', 'è¿”å›',
                       'æ­é–‹', 'é¢å°', 'çªç ´', 'æ±ºå®š', 'é­é‡']

        verb_count = sum(outline.count(verb) for verb in action_verbs)

        if verb_count >= 10:
            return 1.0
        elif verb_count >= 5:
            return 0.7
        elif verb_count >= 3:
            return 0.5
        return 0.3

    def check_complete_names(self, outline: str) -> float:
        """æª¢æŸ¥è§’è‰²åç¨±å®Œæ•´ï¼ˆ0-1ï¼‰"""
        if re.search(r'\*+[\u4e00-\u9fff]', outline) or re.search(r'[\u4e00-\u9fff]\*+', outline):
            return 0.0

        if 'æŸæŸ' in outline or 'æŸå€‹' in outline:
            return 0.3

        chinese_names = re.findall(r'[\u4e00-\u9fff]{2,3}(?=[ï¼Œã€‚ï¼šã€]|$)', outline)
        if len(chinese_names) >= 3:
            return 1.0
        elif len(chinese_names) >= 1:
            return 0.7
        return 0.5

    def check_no_repetition(self, outline: str) -> float:
        """æª¢æŸ¥ç« ç¯€é–“ç„¡é«˜åº¦é‡è¤‡ï¼ˆ0-1ï¼‰"""
        chapters = re.split(r'ç¬¬\d+ç« ', outline)
        if len(chapters) < 3:
            return 0.5

        repetition_count = 0
        common_phrases = ['æ–°çš„', 'é–‹å§‹', 'æ¢ç´¢', 'ç™¼ç¾', 'è§£æ±º', 'é¢å°', 'çªç ´']

        for phrase in common_phrases:
            count = outline.count(phrase)
            if count > 5:
                repetition_count += 1

        if repetition_count > 3:
            return 0.3
        elif repetition_count > 1:
            return 0.7
        return 1.0

    def check_outline_length(self, outline: str) -> float:
        """æª¢æŸ¥å¤§ç¶±é•·åº¦é©ä¸­ï¼ˆ0-1ï¼‰"""
        length = len(outline)

        if 2000 <= length <= 5000:
            return 1.0
        elif 1500 <= length < 2000 or 5000 < length <= 6000:
            return 0.7
        elif 1000 <= length < 1500 or 6000 < length <= 8000:
            return 0.5
        return 0.3

    def check_chapter_length(self, outline: str) -> float:
        """æª¢æŸ¥æ¯ç« æè¿°å……åˆ†ï¼ˆ0-1ï¼‰"""
        chapters = re.split(r'ç¬¬\d+ç« ', outline)
        if len(chapters) < 2:
            return 0.0

        chapters = chapters[1:]

        adequate_count = 0
        for chapter in chapters:
            if len(chapter.strip()) >= 50:
                adequate_count += 1

        ratio = adequate_count / max(len(chapters), 1)
        return ratio

    def save_outline_with_reviews(self, outline: str, params: Dict, score: Dict, index: int):
        """ä¿å­˜å¤§ç¶±ï¼ˆåŒ…å« AI è©•å¯©æ„è¦‹ï¼‰"""
        # ä¿å­˜å¤§ç¶±
        filename = f"{self.output_dir}/outlines/outline_{index:03d}_auto{score['auto_score']:.0f}_ai{score['ai_score']:.0f}.txt"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# åƒæ•¸é…ç½®\n")
            f.write(f"temperature: {params['temperature']}\n")
            f.write(f"top_p: {params['top_p']}\n")
            f.write(f"repetition_penalty: {params['repetition_penalty']}\n")
            f.write(f"max_tokens: {params['max_tokens']}\n")
            f.write(f"\n# å“è³ªè©•åˆ†\n")
            f.write(f"è‡ªå‹•è©•åˆ†: {score['auto_score']:.0f}/100\n")
            f.write(f"AI è©•åˆ†: {score['ai_score']:.0f}/100\n")
            f.write(f"ç¶œåˆåˆ†æ•¸: {score['combined_score']:.1f}/100\n")
            f.write(f"AI ä¸€è‡´æ€§: {score['ai_agreement']}\n")

            # AI è©•å¯©æ„è¦‹
            if score['ai_reviews']:
                f.write(f"\n# AI è©•å¯©æ„è¦‹\n")
                for review in score['ai_reviews']:
                    f.write(f"\n## {review.get('reviewer_name', 'Unknown')}\n")
                    f.write(f"åš´é‡åº¦åˆ†æ•¸: {review['severity_score']}/100\n")
                    f.write(f"ç¼ºé™·æ•¸é‡: {review['flaw_count']}\n")
                    f.write(f"æ•´é«”è©•åƒ¹: {review['verdict']}\n")

                    if review['fatal_flaws']:
                        f.write(f"\nè‡´å‘½ç¼ºé™·:\n")
                        for i, flaw in enumerate(review['fatal_flaws'], 1):
                            f.write(f"  {i}. [{flaw.get('severity', '?')}] {flaw.get('location', '?')}: {flaw.get('issue', '?')}\n")

            f.write(f"\n{'='*60}\n\n")
            f.write(outline)

        # ä¿å­˜ AI è©•å¯©è©³ç´°æ•¸æ“šï¼ˆJSONï¼‰
        if score['ai_reviews']:
            review_filename = f"{self.output_dir}/ai_reviews/review_{index:03d}.json"
            with open(review_filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'params': params,
                    'scores': {
                        'auto': score['auto_score'],
                        'ai': score['ai_score'],
                        'combined': score['combined_score']
                    },
                    'ai_reviews': score['ai_reviews'],
                    'ai_agreement': score['ai_agreement']
                }, f, ensure_ascii=False, indent=2)

    def run_full_test(self):
        """åŸ·è¡Œå®Œæ•´æ¸¬è©¦ï¼ˆæ•´åˆç›¸å°æ’åï¼‰"""
        self.start_time = time.time()

        print("\n" + "="*60)
        print("ğŸ§ª DeepSeek R1 åƒæ•¸è‡ªå‹•æ¸¬è©¦ç³»çµ±ï¼ˆAI è©•å¯©å¢å¼·ç‰ˆï¼‰")
        print("="*60)

        # ç”Ÿæˆåƒæ•¸çµ„åˆ
        param_combinations = self.generate_param_combinations()
        total = len(param_combinations)

        mode_name = "å¿«é€Ÿæ¸¬è©¦" if self.quick_mode else "å®Œæ•´æ¸¬è©¦"
        ai_status = "å•Ÿç”¨" if self.enable_ai_review else "åœç”¨"

        print(f"\næ¨¡å¼: {mode_name}")
        print(f"AI è©•å¯©: {ai_status}")
        print(f"ç¸½æ¸¬è©¦çµ„åˆæ•¸: {total}")

        if self.enable_ai_review:
            print(f"é è¨ˆæ™‚é–“: {total * 1.5:.0f}-{total * 3:.0f} åˆ†é˜ï¼ˆå« AI è©•å¯©ï¼‰")
        else:
            print(f"é è¨ˆæ™‚é–“: {total * 0.5:.0f}-{total * 1:.0f} åˆ†é˜")

        print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # Phase 1: è‡ªå‹•æ¸¬è©¦ + AI è©•å¯©
        logger.info("="*60)
        logger.info("Phase 1: åƒæ•¸æ¸¬è©¦èˆ‡è©•ä¼°")
        logger.info("="*60)

        results_with_outlines = []

        for i, params in enumerate(param_combinations, 1):
            try:
                outline, score = self.test_param_combination(params, i, total)
                results_with_outlines.append((outline, params, score))

                # é¡¯ç¤ºç•¶å‰æœ€ä½³
                if results_with_outlines:
                    best = max(results_with_outlines, key=lambda x: x[2]['combined_score'])
                    print(f"\nğŸ’¡ ç•¶å‰æœ€ä½³: ç¶œåˆåˆ†æ•¸ {best[2]['combined_score']:.1f}/100")
                    print(f"   åƒæ•¸: temp={best[1]['temperature']}, "
                          f"top_p={best[1]['top_p']}, "
                          f"rep={best[1]['repetition_penalty']}, "
                          f"max_tok={best[1]['max_tokens']}")

                # é¡¯ç¤ºé€²åº¦
                elapsed = time.time() - self.start_time
                avg_time = elapsed / i
                remaining = (total - i) * avg_time
                print(f"\nâ±ï¸  é€²åº¦: {i}/{total} ({i/total*100:.1f}%)")
                print(f"   å·²ç”¨æ™‚é–“: {elapsed/60:.1f} åˆ†é˜")
                print(f"   é è¨ˆå‰©é¤˜: {remaining/60:.1f} åˆ†é˜")

            except Exception as e:
                logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()

        # Phase 2: ç›¸å°æ’åï¼ˆTop 10ï¼Œåƒ…åœ¨å•Ÿç”¨ AI è©•å¯©æ™‚åŸ·è¡Œï¼‰
        if self.enable_ai_review and len(results_with_outlines) >= 3:
            logger.info("\n" + "="*60)
            logger.info("Phase 2: ç›¸å°æ’åï¼ˆTop å€™é¸ï¼‰")
            logger.info("="*60)

            # å…ˆæŒ‰ç¶œåˆåˆ†æ•¸æ’åºï¼Œå–å‰ 10ï¼ˆæˆ–å…¨éƒ¨å¦‚æœå°‘æ–¼ 10ï¼‰
            top_n = min(10, len(results_with_outlines))
            top_candidates = sorted(results_with_outlines,
                                   key=lambda x: x[2]['combined_score'],
                                   reverse=True)[:top_n]

            # å…©å…©æ¯”è¼ƒæ’å
            final_ranking = self.rank_by_comparison(top_candidates)

            # æ›´æ–° resultsï¼ˆç”¨æ–¼å ±å‘Šç”Ÿæˆï¼‰
            self.results = [x[2] for x in final_ranking]

            # ä¿å­˜å®Œæ•´æ’åä¿¡æ¯
            for rank, (outline, params, score) in enumerate(final_ranking, 1):
                score['final_rank'] = rank

        else:
            # ä¸é€²è¡Œç›¸å°æ’åï¼Œç›´æ¥æŒ‰ç¶œåˆåˆ†æ•¸æ’åº
            self.results = [x[2] for x in sorted(results_with_outlines,
                                                 key=lambda x: x[2]['combined_score'],
                                                 reverse=True)]

        # Phase 3: ç”Ÿæˆå ±å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
        print("="*60)
        self.generate_enhanced_report()

        total_time = time.time() - self.start_time
        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")

    def generate_enhanced_report(self):
        """ç”Ÿæˆå¢å¼·å ±å‘Š"""
        if not self.results:
            logger.warning("æ²’æœ‰æ¸¬è©¦çµæœ")
            return

        # æ’åºçµæœ
        sorted_results = sorted(self.results, key=lambda x: x['combined_score'], reverse=True)

        # ç”Ÿæˆ Markdown å ±å‘Š
        report = self.build_enhanced_markdown_report(sorted_results)

        # ä¿å­˜å ±å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mode_suffix = "quick" if self.quick_mode else "full"
        ai_suffix = "ai" if self.enable_ai_review else "auto"

        report_path = f"{self.output_dir}/r1_params_test_report_{mode_suffix}_{ai_suffix}_{timestamp}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        # åŒæ™‚ä¿å­˜ç‚ºæœ€æ–°å ±å‘Š
        latest_path = f"{self.output_dir}/r1_params_test_report_latest_{ai_suffix}.md"
        with open(latest_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"\nğŸ“„ å ±å‘Šå·²ç”Ÿæˆ:")
        print(f"   {report_path}")
        print(f"   {latest_path}")

    def build_enhanced_markdown_report(self, sorted_results: List[Dict]) -> str:
        """ç”Ÿæˆå¢å¼· Markdown å ±å‘Š"""
        mode_name = "å¿«é€Ÿæ¸¬è©¦" if self.quick_mode else "å®Œæ•´æ¸¬è©¦"
        ai_status = "AI è©•å¯©" if self.enable_ai_review else "è‡ªå‹•è©•ä¼°"
        total_time = time.time() - self.start_time if self.start_time else 0

        report = f"""# DeepSeek R1 åƒæ•¸æ¸¬è©¦å ±å‘Šï¼ˆ{ai_status}ç‰ˆï¼‰

## æ¸¬è©¦æ™‚é–“
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ¸¬è©¦é…ç½®
- æ¸¬è©¦æ¨¡å¼: {mode_name}
- è©•ä¼°æ–¹å¼: {ai_status}
- ç¸½æ¸¬è©¦çµ„åˆ: {len(self.results)} çµ„
- æ¯çµ„æ¸¬è©¦æ¬¡æ•¸: 1 æ¬¡
- ç¸½ç”Ÿæˆæ¬¡æ•¸: {len(self.results)} æ¬¡
- ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜

"""

        # Top 3 çµæœ
        report += "## ğŸ† æœ€çµ‚æ’å\n\n"

        medals = ['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰']
        rankings = ['å† è»', 'äºè»', 'å­£è»']

        for i, (medal, ranking) in enumerate(zip(medals, rankings)):
            if i >= len(sorted_results):
                break

            result = sorted_results[i]
            params = result['params']
            has_rank = 'final_rank' in result

            rank_info = f" - ç›¸å°æ’å: {result['final_rank']}" if has_rank else ""

            report += f"### {medal} {ranking}{rank_info}\n\n"
            report += "**åƒæ•¸é…ç½®**ï¼š\n"
            report += "```python\n"
            report += "{\n"
            report += f"    'temperature': {params['temperature']},\n"
            report += f"    'top_p': {params['top_p']},\n"
            report += f"    'repetition_penalty': {params['repetition_penalty']},\n"
            report += f"    'max_tokens': {params['max_tokens']}\n"
            report += "}\n"
            report += "```\n\n"

            report += "**è©•åˆ†è©³æƒ…**ï¼š\n"
            report += f"- è‡ªå‹•è©•åˆ†ï¼š{result['auto_score']:.0f}/100\n"

            if self.enable_ai_review and result.get('ai_reviews'):
                report += f"- AI è©•åˆ†ï¼ˆä¸­ä½æ•¸ï¼‰ï¼š{result['ai_score']:.0f}/100\n"
                report += f"- ç¶œåˆåˆ†æ•¸ï¼š{result['combined_score']:.1f}/100\n"
                report += f"- AI ä¸€è‡´æ€§ï¼š{result['ai_agreement']}\n\n"

                report += "**AI è©•å¯©æ„è¦‹**ï¼š\n"
                for review in result['ai_reviews']:
                    reviewer_name = review.get('reviewer_name', 'Unknown')
                    verdict = review.get('verdict', 'ç„¡è©•åƒ¹')
                    report += f"- **{reviewer_name}**: \"{verdict}\"\n"

                report += "\n"

                # è‡´å‘½ç¼ºé™·åŒ¯ç¸½
                all_flaws = []
                for review in result['ai_reviews']:
                    all_flaws.extend(review.get('fatal_flaws', []))

                if all_flaws:
                    report += "**ä¸»è¦ç¼ºé™·**ï¼š\n"
                    for flaw in all_flaws[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                        severity = flaw.get('severity', '?')
                        location = flaw.get('location', '?')
                        issue = flaw.get('issue', '?')
                        report += f"- [{severity}] {location}: {issue}\n"
                    report += "\n"
            else:
                report += f"- ç¸½åˆ†ï¼š{result['auto_score']:.0f}/100\n\n"

        # è©³ç´°æ¸¬è©¦çµæœè¡¨æ ¼
        report += "## ğŸ“Š è©³ç´°æ¸¬è©¦çµæœ\n\n"

        if self.enable_ai_review:
            report += "| æ’å | temp | top_p | rep | max_tok | è‡ªå‹•åˆ† | AIåˆ† | ç¶œåˆåˆ† | ä¸€è‡´æ€§ |\n"
            report += "|------|------|-------|-----|---------|--------|------|--------|--------|\n"

            for i, result in enumerate(sorted_results, 1):
                params = result['params']
                report += f"| {i} | {params['temperature']} | {params['top_p']} | "
                report += f"{params['repetition_penalty']} | {params['max_tokens']} | "
                report += f"{result['auto_score']:.0f} | {result['ai_score']:.0f} | "
                report += f"{result['combined_score']:.1f} | {result['ai_agreement']} |\n"
        else:
            report += "| æ’å | temp | top_p | rep | max_tok | ç¸½åˆ† | æ ¼å¼ | å…§å®¹ | é•·åº¦ |\n"
            report += "|------|------|-------|-----|---------|------|------|------|------|\n"

            for i, result in enumerate(sorted_results, 1):
                params = result['params']
                details = result.get('auto_details', {})
                format_score = (details.get('think_tags', 0) + details.get('star_placeholders', 0) +
                              details.get('dot_placeholders', 0) + details.get('mixed_language', 0)) * 10
                content_score = (details.get('unique_titles', 0) + details.get('concrete_plots', 0) +
                               details.get('complete_names', 0) + details.get('no_repetition', 0)) * 10
                length_score = (details.get('outline_length', 0) + details.get('chapter_length', 0)) * 10

                report += f"| {i} | {params['temperature']} | {params['top_p']} | "
                report += f"{params['repetition_penalty']} | {params['max_tokens']} | "
                report += f"{result['auto_score']:.0f} | {format_score:.0f} | "
                report += f"{content_score:.0f} | {length_score:.0f} |\n"

        report += "\n"

        # åƒæ•¸å½±éŸ¿åˆ†æ
        if self.enable_ai_review:
            report += self.analyze_ai_parameter_impact(sorted_results)
        else:
            report += self.analyze_parameter_impact(sorted_results)

        # å»ºè­°é…ç½®
        if sorted_results:
            best = sorted_results[0]
            params = best['params']

            report += "## ğŸ’¡ å»ºè­°é…ç½®\n\n"
            report += "åŸºæ–¼æ¸¬è©¦çµæœï¼Œå»ºè­°ä½¿ç”¨ï¼š\n\n"
            report += "```python\n"
            report += "'architect': {\n"
            report += f"    'temperature': {params['temperature']},\n"
            report += f"    'top_p': {params['top_p']},\n"
            report += f"    'repetition_penalty': {params['repetition_penalty']},\n"
            report += f"    'max_tokens': {params['max_tokens']}\n"
            report += "}\n"
            report += "```\n\n"

            if self.enable_ai_review:
                report += f"**ç†ç”±**ï¼š\n"
                report += f"- è‡ªå‹•è©•åˆ†ï¼š{best['auto_score']:.0f}/100ï¼ˆå“è³ªæª¢æŸ¥é€šéï¼‰\n"
                report += f"- AI è©•åˆ†ï¼š{best['ai_score']:.0f}/100ï¼ˆå°ˆå®¶ä¸€è‡´èªå¯ï¼‰\n"
                report += f"- ç¶œåˆåˆ†æ•¸ï¼š{best['combined_score']:.1f}/100ï¼ˆæœ€ä½³å¹³è¡¡ï¼‰\n"
                report += f"- AI ä¸€è‡´æ€§ï¼š{best['ai_agreement']}ï¼ˆè©•å¯©æ„è¦‹{best['ai_agreement'] == 'high' and 'é«˜åº¦' or ''}ä¸€è‡´ï¼‰\n\n"

        # æ¸¬è©¦æ•¸æ“šèªªæ˜
        report += "## ğŸ“ æ¸¬è©¦æ•¸æ“š\n\n"
        report += f"æ‰€æœ‰æ¸¬è©¦å¤§ç¶±å·²ä¿å­˜è‡³ï¼š`{self.output_dir}/outlines/`\n\n"
        report += "æª”æ¡ˆå‘½åæ ¼å¼ï¼š`outline_XXX_autoYY_aiZZ.txt`\n"
        report += "- XXX: æ¸¬è©¦åºè™Ÿï¼ˆ001-999ï¼‰\n"
        report += "- YY: è‡ªå‹•è©•åˆ†ï¼ˆ0-100ï¼‰\n"
        report += "- ZZ: AI è©•åˆ†ï¼ˆ0-100ï¼‰\n\n"

        if self.enable_ai_review:
            report += f"AI è©•å¯©è©³ç´°æ•¸æ“šï¼š`{self.output_dir}/ai_reviews/`\n\n"

        return report

    def analyze_parameter_impact(self, sorted_results: List[Dict]) -> str:
        """åˆ†æåƒæ•¸å½±éŸ¿ï¼ˆè‡ªå‹•è©•ä¼°ç‰ˆï¼‰"""
        report = "## ğŸ“ˆ åƒæ•¸å½±éŸ¿åˆ†æ\n\n"

        # æŒ‰åƒæ•¸åˆ†çµ„çµ±è¨ˆ
        temp_groups = {}
        topp_groups = {}
        rep_groups = {}
        tok_groups = {}

        for result in sorted_results:
            params = result['params']
            score = result['auto_score']

            # Temperature
            temp = params['temperature']
            if temp not in temp_groups:
                temp_groups[temp] = []
            temp_groups[temp].append(score)

            # Top_P
            topp = params['top_p']
            if topp not in topp_groups:
                topp_groups[topp] = []
            topp_groups[topp].append(score)

            # Repetition Penalty
            rep = params['repetition_penalty']
            if rep not in rep_groups:
                rep_groups[rep] = []
            rep_groups[rep].append(score)

            # Max Tokens
            tok = params['max_tokens']
            if tok not in tok_groups:
                tok_groups[tok] = []
            tok_groups[tok].append(score)

        # Temperature å½±éŸ¿
        report += "### Temperature å½±éŸ¿\n\n"
        for temp in sorted(temp_groups.keys()):
            scores = temp_groups[temp]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(temp_groups[t])/len(temp_groups[t]) for t in temp_groups) else ""
            report += f"- **{temp}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # Top_P å½±éŸ¿
        report += "### Top_P å½±éŸ¿\n\n"
        for topp in sorted(topp_groups.keys()):
            scores = topp_groups[topp]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(topp_groups[t])/len(topp_groups[t]) for t in topp_groups) else ""
            report += f"- **{topp}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # Repetition Penalty å½±éŸ¿
        report += "### Repetition Penalty å½±éŸ¿\n\n"
        for rep in sorted(rep_groups.keys()):
            scores = rep_groups[rep]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(rep_groups[r])/len(rep_groups[r]) for r in rep_groups) else ""
            report += f"- **{rep}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # Max Tokens å½±éŸ¿
        report += "### Max Tokens å½±éŸ¿\n\n"
        for tok in sorted(tok_groups.keys()):
            scores = tok_groups[tok]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(tok_groups[t])/len(tok_groups[t]) for t in tok_groups) else ""
            report += f"- **{tok}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        return report

    def analyze_ai_parameter_impact(self, sorted_results: List[Dict]) -> str:
        """åˆ†æåƒæ•¸å½±éŸ¿ï¼ˆAI è©•å¯©ç‰ˆï¼‰"""
        report = "## ğŸ“ˆ åƒæ•¸å½±éŸ¿åˆ†æï¼ˆAI è©•å¯©ï¼‰\n\n"

        # æŒ‰åƒæ•¸åˆ†çµ„çµ±è¨ˆï¼ˆä½¿ç”¨ç¶œåˆåˆ†æ•¸ï¼‰
        temp_groups = {}
        topp_groups = {}
        rep_groups = {}
        tok_groups = {}

        for result in sorted_results:
            params = result['params']
            score = result['combined_score']

            # Temperature
            temp = params['temperature']
            if temp not in temp_groups:
                temp_groups[temp] = []
            temp_groups[temp].append(score)

            # Top_P
            topp = params['top_p']
            if topp not in topp_groups:
                topp_groups[topp] = []
            topp_groups[topp].append(score)

            # Repetition Penalty
            rep = params['repetition_penalty']
            if rep not in rep_groups:
                rep_groups[rep] = []
            rep_groups[rep].append(score)

            # Max Tokens
            tok = params['max_tokens']
            if tok not in tok_groups:
                tok_groups[tok] = []
            tok_groups[tok].append(score)

        # Temperature å½±éŸ¿
        report += "### Temperature å½±éŸ¿ï¼ˆç¶œåˆåˆ†æ•¸ï¼‰\n\n"
        for temp in sorted(temp_groups.keys()):
            scores = temp_groups[temp]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(temp_groups[t])/len(temp_groups[t]) for t in temp_groups) else ""
            report += f"- **{temp}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # Top_P å½±éŸ¿
        report += "### Top_P å½±éŸ¿ï¼ˆç¶œåˆåˆ†æ•¸ï¼‰\n\n"
        for topp in sorted(topp_groups.keys()):
            scores = topp_groups[topp]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(topp_groups[t])/len(topp_groups[t]) for t in topp_groups) else ""
            report += f"- **{topp}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # Repetition Penalty å½±éŸ¿
        report += "### Repetition Penalty å½±éŸ¿ï¼ˆç¶œåˆåˆ†æ•¸ï¼‰\n\n"
        for rep in sorted(rep_groups.keys()):
            scores = rep_groups[rep]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(rep_groups[r])/len(rep_groups[r]) for r in rep_groups) else ""
            report += f"- **{rep}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # Max Tokens å½±éŸ¿
        report += "### Max Tokens å½±éŸ¿ï¼ˆç¶œåˆåˆ†æ•¸ï¼‰\n\n"
        for tok in sorted(tok_groups.keys()):
            scores = tok_groups[tok]
            avg_score = sum(scores) / len(scores)
            best_mark = " â­" if avg_score == max(sum(tok_groups[t])/len(tok_groups[t]) for t in tok_groups) else ""
            report += f"- **{tok}**: å¹³å‡åˆ† {avg_score:.1f}{best_mark}\n"
        report += "\n"

        # AI è©•å¯©ä¸€è‡´æ€§åˆ†æ
        report += "### AI è©•å¯©ä¸€è‡´æ€§åˆ†æ\n\n"
        agreement_counts = {'high': 0, 'medium': 0, 'low': 0, 'N/A': 0}

        for result in sorted_results:
            agreement = result.get('ai_agreement', 'N/A')
            agreement_counts[agreement] = agreement_counts.get(agreement, 0) + 1

        total = sum(agreement_counts.values())
        for agreement, count in sorted(agreement_counts.items()):
            if count > 0:
                percentage = count / total * 100
                report += f"- **{agreement}**: {count} æ¬¡ ({percentage:.1f}%)\n"

        report += "\n"

        return report


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='DeepSeek R1 åƒæ•¸è‡ªå‹•æ¸¬è©¦ç³»çµ±ï¼ˆAI è©•å¯©å¢å¼·ç‰ˆï¼‰')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆ10çµ„é—œéµåƒæ•¸ï¼‰')
    parser.add_argument('--full', action='store_true', help='å®Œæ•´æ¸¬è©¦æ¨¡å¼ï¼ˆæ‰€æœ‰çµ„åˆï¼‰')
    parser.add_argument('--no-ai', action='store_true', help='åœç”¨ AI è©•å¯©ï¼ˆåƒ…è‡ªå‹•è©•ä¼°ï¼‰')
    args = parser.parse_args()

    # åŠ è¼‰ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    api_key = os.getenv('SILICONFLOW_API_KEY')

    if not api_key:
        print("âŒ éŒ¯èª¤: æœªæª¢æ¸¬åˆ° SILICONFLOW_API_KEY")
        print("è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½® API Key")
        return

    # ç¢ºå®šæ¸¬è©¦æ¨¡å¼
    if args.full:
        quick_mode = False
    elif args.quick:
        quick_mode = True
    else:
        # é»˜èªä½¿ç”¨å¿«é€Ÿæ¨¡å¼
        quick_mode = True
        print("ğŸ’¡ æç¤º: æœªæŒ‡å®šæ¨¡å¼ï¼Œä½¿ç”¨å¿«é€Ÿæ¸¬è©¦æ¨¡å¼")
        print("   ä½¿ç”¨ --full å¯é€²è¡Œå®Œæ•´æ¸¬è©¦ï¼ˆæ‰€æœ‰ 240 çµ„çµ„åˆï¼‰")
        print("   ä½¿ç”¨ --quick æ˜ç¢ºæŒ‡å®šå¿«é€Ÿæ¸¬è©¦")
        print("   ä½¿ç”¨ --no-ai åœç”¨ AI è©•å¯©\n")

    # AI è©•å¯©ç‹€æ…‹
    enable_ai = not args.no_ai

    if not enable_ai:
        print("âš ï¸  AI è©•å¯©å·²åœç”¨ï¼Œåƒ…ä½¿ç”¨è‡ªå‹•è©•ä¼°")

    # å‰µå»ºæ¸¬è©¦å™¨ä¸¦é‹è¡Œ
    tester = R1ParamsTesterEnhanced(api_key, quick_mode=quick_mode, enable_ai_review=enable_ai)
    tester.run_full_test()


if __name__ == "__main__":
    main()
